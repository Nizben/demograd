{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from demograd.tensor_engine import Tensor\n",
    "from demograd.nn import Linear, Sequential\n",
    "from demograd.optimizers import SGD \n",
    "from demograd.activations import ReLU\n",
    "from demograd.losses import MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "input_dim = 3\n",
    "output_dim = 1\n",
    "X_data = np.random.randn(N, input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_W = np.array([[2.0], [-3.0], [1.0]])\n",
    "true_b = 0.5\n",
    "y_data = X_data.dot(true_W) + true_b\n",
    "y_data += 0.1 * np.random.randn(N, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Tensor(X_data, requires_grad=False)\n",
    "y = Tensor(y_data, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 10\n",
    "model = Sequential(\n",
    "    Linear(input_dim, hidden_dim),\n",
    "    ReLU.apply, \n",
    "    Linear(hidden_dim, output_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(model.parameters, lr=0.01)\n",
    "criterion = MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 13.688759803771973\n",
      "Epoch 100, Loss: 8.828271865844727\n",
      "Epoch 200, Loss: 0.23633918166160583\n",
      "Epoch 300, Loss: 0.1829787790775299\n",
      "Epoch 400, Loss: 0.17365796864032745\n",
      "Epoch 500, Loss: 0.16507560014724731\n",
      "Epoch 600, Loss: 0.15632493793964386\n",
      "Epoch 700, Loss: 0.14454731345176697\n",
      "Epoch 800, Loss: 0.12159093469381332\n",
      "Epoch 900, Loss: 0.09968511015176773\n",
      "Epoch 1000, Loss: 0.07287959009408951\n",
      "Epoch 1100, Loss: 0.048231787979602814\n",
      "Epoch 1200, Loss: 0.03181413933634758\n",
      "Epoch 1300, Loss: 0.022816376760601997\n",
      "Epoch 1400, Loss: 0.01827048510313034\n",
      "Epoch 1500, Loss: 0.015715520828962326\n",
      "Epoch 1600, Loss: 0.014395296573638916\n",
      "Epoch 1700, Loss: 0.013595200143754482\n",
      "Epoch 1800, Loss: 0.01300035323947668\n",
      "Epoch 1900, Loss: 0.01255409698933363\n",
      "Epoch 2000, Loss: 0.012172075919806957\n",
      "Epoch 2100, Loss: 0.011852797120809555\n",
      "Epoch 2200, Loss: 0.011579148471355438\n",
      "Epoch 2300, Loss: 0.011355503462255001\n",
      "Epoch 2400, Loss: 0.011159765534102917\n",
      "Epoch 2500, Loss: 0.010989128611981869\n",
      "Epoch 2600, Loss: 0.01085070800036192\n",
      "Epoch 2700, Loss: 0.010744910687208176\n",
      "Epoch 2800, Loss: 0.010653003118932247\n",
      "Epoch 2900, Loss: 0.010535717010498047\n",
      "Epoch 3000, Loss: 0.010391450487077236\n",
      "Epoch 3100, Loss: 0.010269640944898129\n",
      "Epoch 3200, Loss: 0.010159247554838657\n",
      "Epoch 3300, Loss: 0.010064743459224701\n",
      "Epoch 3400, Loss: 0.009982176125049591\n",
      "Epoch 3500, Loss: 0.009907395578920841\n",
      "Epoch 3600, Loss: 0.009838162921369076\n",
      "Epoch 3700, Loss: 0.009769110940396786\n",
      "Epoch 3800, Loss: 0.00969140324741602\n",
      "Epoch 3900, Loss: 0.009626234881579876\n",
      "Epoch 4000, Loss: 0.00957860890775919\n",
      "Epoch 4100, Loss: 0.009535410441458225\n",
      "Epoch 4200, Loss: 0.009495631791651249\n",
      "Epoch 4300, Loss: 0.009459328837692738\n",
      "Epoch 4400, Loss: 0.009426075965166092\n",
      "Epoch 4500, Loss: 0.009396173991262913\n",
      "Epoch 4600, Loss: 0.009369246661663055\n",
      "Epoch 4700, Loss: 0.009344395250082016\n",
      "Epoch 4800, Loss: 0.00929656345397234\n",
      "Epoch 4900, Loss: 0.009248742833733559\n",
      "Epoch 5000, Loss: 0.009204788133502007\n",
      "Epoch 5100, Loss: 0.009164220653474331\n",
      "Epoch 5200, Loss: 0.009126813150942326\n",
      "Epoch 5300, Loss: 0.009092223830521107\n",
      "Epoch 5400, Loss: 0.009060057811439037\n",
      "Epoch 5500, Loss: 0.009025216102600098\n",
      "Epoch 5600, Loss: 0.008983292616903782\n",
      "Epoch 5700, Loss: 0.008944527246057987\n",
      "Epoch 5800, Loss: 0.008909723721444607\n",
      "Epoch 5900, Loss: 0.008877499960362911\n",
      "Epoch 6000, Loss: 0.008847301825881004\n",
      "Epoch 6100, Loss: 0.008828263729810715\n",
      "Epoch 6200, Loss: 0.008815628476440907\n",
      "Epoch 6300, Loss: 0.00880537647753954\n",
      "Epoch 6400, Loss: 0.00879574753344059\n",
      "Epoch 6500, Loss: 0.008786531165242195\n",
      "Epoch 6600, Loss: 0.008777746930718422\n",
      "Epoch 6700, Loss: 0.008769383653998375\n",
      "Epoch 6800, Loss: 0.008761336095631123\n",
      "Epoch 6900, Loss: 0.008753543719649315\n",
      "Epoch 7000, Loss: 0.00874602422118187\n",
      "Epoch 7100, Loss: 0.008738777600228786\n",
      "Epoch 7200, Loss: 0.008731984533369541\n",
      "Epoch 7300, Loss: 0.008725346066057682\n",
      "Epoch 7400, Loss: 0.008718973957002163\n",
      "Epoch 7500, Loss: 0.008712660521268845\n",
      "Epoch 7600, Loss: 0.008706670254468918\n",
      "Epoch 7700, Loss: 0.008700720965862274\n",
      "Epoch 7800, Loss: 0.008695037104189396\n",
      "Epoch 7900, Loss: 0.008689505979418755\n",
      "Epoch 8000, Loss: 0.008684070780873299\n",
      "Epoch 8100, Loss: 0.008678765967488289\n",
      "Epoch 8200, Loss: 0.00867355614900589\n",
      "Epoch 8300, Loss: 0.008668553084135056\n",
      "Epoch 8400, Loss: 0.008663593791425228\n",
      "Epoch 8500, Loss: 0.008658762089908123\n",
      "Epoch 8600, Loss: 0.00865396298468113\n",
      "Epoch 8700, Loss: 0.008649274706840515\n",
      "Epoch 8800, Loss: 0.008644710294902325\n",
      "Epoch 8900, Loss: 0.008640246465802193\n",
      "Epoch 9000, Loss: 0.00863584689795971\n",
      "Epoch 9100, Loss: 0.008631574921309948\n",
      "Epoch 9200, Loss: 0.008627546019852161\n",
      "Epoch 9300, Loss: 0.008623667061328888\n",
      "Epoch 9400, Loss: 0.008619784377515316\n",
      "Epoch 9500, Loss: 0.008616097271442413\n",
      "Epoch 9600, Loss: 0.008612474426627159\n",
      "Epoch 9700, Loss: 0.008608927950263023\n",
      "Epoch 9800, Loss: 0.00860549882054329\n",
      "Epoch 9900, Loss: 0.008602066896855831\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass: compute predictions.\n",
    "    y_pred = model(X)\n",
    "    # Compute loss.\n",
    "    loss = criterion(y_pred, y)\n",
    "\n",
    "    # Zero the gradients.\n",
    "    optimizer.zero_grad()\n",
    "    # Backward pass.\n",
    "    loss.backward()\n",
    "    # Update parameters.\n",
    "    optimizer.step()\n",
    "\n",
    "    # Optionally, print the loss every 100 epochs.\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
